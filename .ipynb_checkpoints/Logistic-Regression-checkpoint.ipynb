{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tagging using Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Initialisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/arghyabhattacharya/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/arghyabhattacharya/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = brown.tagged_sents(tagset='universal')\n",
    "_N = int(len(sents) * (8 / 10))\n",
    "sents_train = sents[:_N]\n",
    "sents_test = sents[_N:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DET'),\n",
       " ('Fulton', 'NOUN'),\n",
       " ('County', 'NOUN'),\n",
       " ('Grand', 'ADJ'),\n",
       " ('Jury', 'NOUN'),\n",
       " ('said', 'VERB'),\n",
       " ('Friday', 'NOUN'),\n",
       " ('an', 'DET'),\n",
       " ('investigation', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " (\"Atlanta's\", 'NOUN'),\n",
       " ('recent', 'ADJ'),\n",
       " ('primary', 'NOUN'),\n",
       " ('election', 'NOUN'),\n",
       " ('produced', 'VERB'),\n",
       " ('``', '.'),\n",
       " ('no', 'DET'),\n",
       " ('evidence', 'NOUN'),\n",
       " (\"''\", '.'),\n",
       " ('that', 'ADP'),\n",
       " ('any', 'DET'),\n",
       " ('irregularities', 'NOUN'),\n",
       " ('took', 'VERB'),\n",
       " ('place', 'NOUN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One vs Many Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multiclass_LR:\n",
    "    def tag_sent(self, sent):\n",
    "        sent_tagged = []\n",
    "        for i, (word, tag) in enumerate(sent):\n",
    "            x = np.array([int(ex(sent, i)) for k, ex in self.feature_extractors.items()])\n",
    "            max_tag = max([(word, tag) for tag in self.tags], key=lambda k: np.dot(np.append(x, 1), W[k[1]]))\n",
    "            sent_tagged.append(max_tag)\n",
    "        return sent_tagged\n",
    "        \n",
    "    def train_model(self):\n",
    "        for tag in self.tags:\n",
    "            print('Training for tag', tag)\n",
    "            self.W[tag] = self.logistic_regression(tag)\n",
    "    \n",
    "    def sigmoid(self, a):\n",
    "        return 1.0 / (1 + np.exp(-a))\n",
    "        \n",
    "    def logistic_regression(self, tag, num_steps=300000, learning_rate=5e-5, add_intercept=True):\n",
    "        X = self.X[tag]\n",
    "        Y = self.Y[tag]\n",
    "        if add_intercept:\n",
    "            intercept = np.ones((X.shape[0], 1))\n",
    "            X = np.hstack((X, intercept))\n",
    "            \n",
    "        w = np.zeros(X.shape[1])\n",
    "        for step in range(num_steps):\n",
    "            scores = np.dot(X, w)\n",
    "            predictions = self.sigmoid(scores)\n",
    "            output_error_signal = Y - predictions\n",
    "            gradient = np.dot(X.T, output_error_signal)\n",
    "            w += learning_rate * gradient\n",
    "        return w\n",
    "        \n",
    "    def loss(self, w, X, Y):\n",
    "        w = np.array(w).reshape(-1, 1)\n",
    "        sum = 0\n",
    "        for i in range(Y.shape[0]):\n",
    "            x = X[i].reshape(-1, 1)\n",
    "            wTx = np.matmul(w.T, x)\n",
    "            sum += Y[i] * wTx - np.log(1 + np.exp(wTx))\n",
    "        return sum.reshape(())\n",
    "        \n",
    "    def get_feature_set(self):\n",
    "        self.puncs = {'!', \"'\", \"''\", '(', ')', ',', '--', '.', ':', ';', '?', '[', ']', '``'}\n",
    "        self.feature_extractors = {\n",
    "            'is_upper_first_letter': lambda sent, i: sent[i][0][0].isupper(),\n",
    "            'is_upper_all': lambda sent, i: sent[i][0].isupper(),\n",
    "            'is_hyphenated': lambda sent, i: '-' in sent[i][0],\n",
    "            'is_punctuation': lambda sent, i: sent[i][0] in self.puncs,\n",
    "            'current_tag': lambda sent, i: self.tags.index(sent[i][1]),\n",
    "            'previous_tag': lambda sent, i: self.tags.index(sent[i-1][1]) if i != 0 else -1,\n",
    "        }\n",
    "        \n",
    "        for t in range(len(self.tags)):\n",
    "            self.X[self.tags[t]] = []\n",
    "            self.Y[self.tags[t]] = []\n",
    "            for sent in self.sents:\n",
    "                for i, (word, tag) in enumerate(sent):\n",
    "                    \n",
    "                    x = [int(ex(sent, i)) for k, ex in self.feature_extractors.items()]\n",
    "                    print(\"this is the x\", x)\n",
    "                    self.X[self.tags[t]].append(x)\n",
    "                    self.Y[self.tags[t]].append(1 if tag == self.tags[t] else 0)\n",
    "            self.X[self.tags[t]] = np.array(self.X[self.tags[t]])\n",
    "            self.Y[self.tags[t]] = np.array(self.Y[self.tags[t]])\n",
    "\n",
    "    def __init__(self, tagged_sents):\n",
    "        self.sents = tagged_sents\n",
    "        self.tags = list(set([tag for sent in self.sents for word, tag in sent]))\n",
    "        self.X = {}\n",
    "        self.Y = {}\n",
    "        self.W = {}\n",
    "        self.get_feature_set()\n",
    "        self.train_model()\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for tag NOUN\n",
      "Training for tag PRT\n",
      "Training for tag .\n"
     ]
    }
   ],
   "source": [
    "T = Multiclass_LR(sents_train[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68.8615061499447\n"
     ]
    }
   ],
   "source": [
    "score = 0\n",
    "total_count = 0\n",
    "for i, sent in enumerate(sents_test):\n",
    "    try:\n",
    "        res = T.tag_sent(sent)\n",
    "        N = len(sent)\n",
    "        total_count += N\n",
    "        for i in range(N):\n",
    "            score += 1 if res[i][1] == sent[i][1] else 0\n",
    "    except ValueError:\n",
    "        pass\n",
    "            \n",
    "accuracy = (score / total_count) * 100\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
